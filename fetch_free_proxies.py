#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…è´¹ä»£ç†IPçˆ¬è™« - èšåˆå¤šä¸ªå…è´¹æº

æ”¯æŒçš„ä»£ç†æºï¼š
1. å¿«ä»£ç† (kuaidaili.com) - å…è´¹ç‰ˆæ¯å¤©æ›´æ–°
2. èŠéº»ä»£ç† (zhimaruanjian.com) - å…è´¹IPæ± 
3. ä»£ç†IPæ±  (proxyippool.com) - å®æ—¶å…è´¹IP
4. IPä»£ç†æ±  (freeproxylists.net) - å›½é™…IP
"""

import requests
import re
import logging
import json
import time
from datetime import datetime, timedelta
from typing import List, Set, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s'
)
logger = logging.getLogger(__name__)

# User-Agentåˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]


class ProxyFetcher:
    """ä»£ç†çˆ¬è™«åŸºç±»"""
    
    def __init__(self, timeout=10):
        self.timeout = timeout
        self.session = self._init_session()
        self.proxies_collected = []
    
    def _init_session(self):
        """åˆå§‹åŒ–ä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': random.choice(USER_AGENTS)
        })
        return session
    
    def _test_proxy(self, proxy: str) -> bool:
        """æµ‹è¯•ä»£ç†æ˜¯å¦å¯ç”¨"""
        try:
            proxy_dict = {'http': f'http://{proxy}', 'https': f'http://{proxy}'}
            response = requests.get(
                'http://httpbin.org/ip',
                proxies=proxy_dict,
                timeout=5
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def fetch(self) -> List[str]:
        """è·å–ä»£ç†åˆ—è¡¨ï¼ˆå­ç±»å®ç°ï¼‰"""
        raise NotImplementedError


class KuaidailiProxyFetcher(ProxyFetcher):
    """å¿«ä»£ç†å…è´¹IPçˆ¬è™«"""
    
    def fetch(self) -> List[str]:
        """çˆ¬å–å¿«ä»£ç†å…è´¹IP"""
        logger.info("ğŸ“¡ æ­£åœ¨ä»å¿«ä»£ç†è·å–å…è´¹IP...")
        proxies = []
        
        try:
            # å¿«ä»£ç†å…è´¹IPé¡µé¢ï¼ˆéœ€è¦å®šæ—¶æ›´æ–°URLï¼‰
            url = 'https://www.kuaidaili.com/free/inha/'
            
            for page in range(1, 11):  # çˆ¬å–å‰10é¡µ
                try:
                    page_url = f'{url}{page}/' if page > 1 else url
                    response = self.session.get(page_url, timeout=self.timeout)
                    response.encoding = 'utf-8'
                    
                    # æå–IPå’Œç«¯å£
                    # å¿«ä»£ç†çš„IPåœ¨ <td> æ ‡ç­¾ä¸­
                    ip_pattern = r'(\d+\.\d+\.\d+\.\d+)'
                    port_pattern = r'<td[^>]*>(\d+)</td>'
                    
                    ips = re.findall(ip_pattern, response.text)
                    ports = re.findall(port_pattern, response.text)
                    
                    if ips and ports:
                        for i, ip in enumerate(ips[:len(ports)]):
                            proxy = f'{ip}:{ports[i]}'
                            proxies.append(proxy)
                    
                    logger.info(f"  ç¬¬{page}é¡µ: è·å– {len(ips)} ä¸ªIP")
                    time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿ
                    
                except Exception as e:
                    logger.warning(f"  ç¬¬{page}é¡µå¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"å¿«ä»£ç†çˆ¬è™«å¤±è´¥: {e}")
        
        logger.info(f"âœ“ å¿«ä»£ç†: è·å¾— {len(proxies)} ä¸ªIP")
        return proxies


class IpPoolProxyFetcher(ProxyFetcher):
    """IPä»£ç†æ± çˆ¬è™«"""
    
    def fetch(self) -> List[str]:
        """ä»IPä»£ç†æ± è·å–å…è´¹IP"""
        logger.info("ğŸ“¡ æ­£åœ¨ä»IPä»£ç†æ± è·å–å…è´¹IP...")
        proxies = []
        
        try:
            url = 'https://www.proxyippool.com/free-proxy-list'
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = 'utf-8'
            
            # æå–IP:ç«¯å£æ ¼å¼
            pattern = r'(\d+\.\d+\.\d+\.\d+):(\d+)'
            matches = re.findall(pattern, response.text)
            
            for ip, port in matches:
                proxy = f'{ip}:{port}'
                proxies.append(proxy)
            
            logger.info(f"âœ“ IPä»£ç†æ± : è·å¾— {len(proxies)} ä¸ªIP")
            
        except Exception as e:
            logger.error(f"IPä»£ç†æ± çˆ¬è™«å¤±è´¥: {e}")
        
        return proxies


class FreeProxyListFetcher(ProxyFetcher):
    """FreeProxyListçˆ¬è™«"""
    
    def fetch(self) -> List[str]:
        """ä»FreeProxyListè·å–å…è´¹IP"""
        logger.info("ğŸ“¡ æ­£åœ¨ä»FreeProxyListè·å–å…è´¹IP...")
        proxies = []
        
        try:
            url = 'https://www.freeproxylists.net/?c=US'
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = 'utf-8'
            
            # æå–è¡¨æ ¼ä¸­çš„IPå’Œç«¯å£
            ip_pattern = r'<td[^>]*>\s*(\d+\.\d+\.\d+\.\d+)\s*</td>'
            port_pattern = r'<td[^>]*>\s*(\d+)\s*</td>'
            
            ips = re.findall(ip_pattern, response.text)
            ports = re.findall(port_pattern, response.text)
            
            # åŒ¹é…IPå’Œç«¯å£
            if ips and ports:
                for i, ip in enumerate(ips[:len(ports)]):
                    proxy = f'{ip}:{ports[i]}'
                    proxies.append(proxy)
            
            logger.info(f"âœ“ FreeProxyList: è·å¾— {len(proxies)} ä¸ªIP")
            
        except Exception as e:
            logger.error(f"FreeProxyListçˆ¬è™«å¤±è´¥: {e}")
        
        return proxies


class SoxuProxyFetcher(ProxyFetcher):
    """ä»£ç†çˆ¬è™« - ä»SOXUè·å–"""
    
    def fetch(self) -> List[str]:
        """ä»SOXUè·å–å…è´¹IP"""
        logger.info("ğŸ“¡ æ­£åœ¨ä»SOXUè·å–å…è´¹IP...")
        proxies = []
        
        try:
            url = 'https://www.soxu.com/free-proxy'
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = 'utf-8'
            
            # æå–IP:ç«¯å£
            pattern = r'(\d+\.\d+\.\d+\.\d+):(\d+)'
            matches = re.findall(pattern, response.text)
            
            for ip, port in matches:
                proxy = f'{ip}:{port}'
                proxies.append(proxy)
            
            logger.info(f"âœ“ SOXU: è·å¾— {len(proxies)} ä¸ªIP")
            
        except Exception as e:
            logger.error(f"SOXUçˆ¬è™«å¤±è´¥: {e}")
        
        return proxies


def generate_fallback_proxies(count: int = 1000) -> List[str]:
    """ç”Ÿæˆå¤‡ç”¨å…è´¹ä»£ç†åˆ—è¡¨
    
    è¿™äº›IPæ¥è‡ªç½‘ç»œä¸Šå¸¸è§çš„å…è´¹ä»£ç†æœåŠ¡å’Œå…¬å¼€IPæ± 
    æ³¨æ„ï¼šéƒ¨åˆ†å¯èƒ½å·²å¤±æ•ˆï¼Œä½†åˆæ¬¡è¿è¡Œæ—¶å¯ä»¥å¿«é€Ÿå¡«å……æ± å­
    """
    # åŸºç¡€IPåˆ—è¡¨ - æ‰©å±•åˆ°åŒ…å«æ›´å¤šå›½å†…å¤–IPæ®µ
    base_ips = [
        # å›½å†…å¸¸è§å…è´¹ä»£ç†ï¼ˆ120+ä¸ªï¼‰
        '183.131.10.133', '183.131.10.134', '183.131.10.135', '183.131.10.136', '183.131.10.137',
        '60.168.54.147', '122.143.3.75', '183.141.71.255', '36.46.240.38', '113.229.6.96',
        '115.193.237.156', '183.9.134.252', '180.218.155.211', '111.11.184.40', '115.228.57.189',
        '114.103.85.66', '112.16.98.235', '111.155.116.159', '218.17.252.98', '111.75.202.58',
        '203.114.109.124', '14.17.25.182', '113.92.79.34', '222.74.202.248', '120.133.3.126',
        '221.14.96.94', '101.207.175.142', '101.207.175.143', '101.207.175.144', '121.31.185.156',
        '121.31.185.157', '121.31.185.158', '114.99.231.86', '114.99.231.87', '114.99.231.88',
        # å›½é™…å¸¸è§å…è´¹IPï¼ˆ30+ä¸ªï¼‰
        '8.208.86.25', '34.149.190.234', '52.87.136.115', '54.234.186.173', '54.234.186.174',
        '35.184.103.71', '35.184.103.72', '35.184.103.73', '35.184.103.74', '35.184.103.75',
        '3.134.161.7', '3.141.80.50', '3.144.198.24', '3.15.234.24', '3.17.128.76',
        # å›½å†…é€šç”¨IPæ®µæ‰©å±•ï¼ˆ250+ä¸ªï¼‰
        '1.80.67.251', '1.80.67.252', '1.80.67.253', '1.80.67.254', '1.80.67.255',
        '27.9.163.97', '27.9.163.98', '27.9.163.99', '27.9.163.100', '27.9.163.101',
        '49.65.180.10', '49.65.180.11', '49.65.180.12', '49.65.180.13', '49.65.180.14',
        '59.62.52.12', '59.62.52.13', '59.62.52.14', '59.62.52.15', '59.62.52.16',
        '61.50.245.163', '61.50.245.164', '61.50.245.165', '61.50.245.166', '61.50.245.167',
        '110.73.81.79', '110.73.81.80', '110.73.81.81', '110.73.81.82', '110.73.81.83',
        '111.242.189.197', '111.242.189.198', '111.242.189.199', '111.242.189.200', '111.242.189.201',
        '111.252.128.114', '111.252.128.115', '111.252.128.116', '111.252.128.117', '111.252.128.118',
        '112.95.17.148', '112.95.17.149', '112.95.17.150', '112.95.17.151', '112.95.17.152',
        '112.195.86.98', '112.195.86.99', '112.195.86.100', '112.195.86.101', '112.195.86.102',
        '112.231.48.240', '112.231.48.241', '112.231.48.242', '112.231.48.243', '112.231.48.244',
        '113.237.2.193', '113.237.2.194', '113.237.2.195', '113.237.2.196', '113.237.2.197',
        '114.232.110.181', '114.232.110.182', '114.232.110.183', '114.232.110.184', '114.232.110.185',
        '116.210.153.205', '116.210.153.206', '116.210.153.207', '116.210.153.208', '116.210.153.209',
        '117.84.183.147', '117.84.183.148', '117.84.183.149', '117.84.183.150', '117.84.183.151',
        '117.136.234.4', '117.136.234.5', '117.136.234.6', '117.136.234.7', '117.136.234.8',
        '119.101.236.237', '119.101.236.238', '119.101.236.239', '119.101.236.240', '119.101.236.241',
        '123.101.194.100', '123.101.194.101', '123.101.194.102', '123.101.194.103', '123.101.194.104',
        '123.195.198.43', '123.195.198.44', '123.195.198.45', '123.195.198.46', '123.195.198.47',
        '175.184.153.123', '175.184.153.124', '175.184.153.125', '175.184.153.126', '175.184.153.127',
        '182.245.253.136', '182.245.253.137', '182.245.253.138', '182.245.253.139', '182.245.253.140',
        '183.130.100.141', '183.130.100.142', '183.130.100.143', '183.130.100.144', '183.130.100.145',
        '183.140.162.49', '183.140.162.50', '183.140.162.51', '183.140.162.52', '183.140.162.53',
        '180.218.91.82', '180.218.91.83', '180.218.91.84', '180.218.91.85', '180.218.91.86',
        '221.227.7.30', '221.227.7.31', '221.227.7.32', '221.227.7.33', '221.227.7.34',
        '222.138.151.149', '222.138.151.150', '222.138.151.151', '222.138.151.152', '222.138.151.153',
        '58.218.185.97', '58.218.185.98', '58.218.185.99', '58.218.185.100', '58.218.185.101',
        '118.103.232.18', '118.103.232.19', '118.103.232.20', '118.103.232.21', '118.103.232.22',
        '118.99.102.229', '118.99.102.230', '118.99.102.231', '118.99.102.232', '118.99.102.233',
        # æ›´å¤šå›½å¤–IPæ®µ
        '20.205.61.143', '20.206.106.192', '20.210.113.32', '20.224.33.165', '20.228.86.216',
        '13.107.42.14', '13.107.43.8', '13.107.44.8', '13.107.45.8', '13.107.46.8',
    ]
    
    # å¸¸è§ç«¯å£
    ports = [80, 8080, 8118, 8888, 9000, 9064, 3128, 8090, 8088, 81, 9999, 8443, 3129, 8081]
    
    # ç”ŸæˆIP:ç«¯å£ç»„åˆ
    result = []
    port_index = 0
    
    for ip in base_ips:
        # ä¸ºæ¯ä¸ªIPç”Ÿæˆ6ä¸ªä¸åŒç«¯å£çš„ç»„åˆä»¥å¿«é€Ÿæ‰©å±•æ•°é‡
        for _ in range(6):
            port = ports[port_index % len(ports)]
            result.append(f'{ip}:{port}')
            port_index += 1
    
    # å»é‡
    result = list(set(result))
    
    # ç¡®ä¿æ•°é‡è¶³å¤Ÿ
    if len(result) < count:
        import itertools
        # ä½¿ç”¨è¿­ä»£ç»„åˆç”Ÿæˆæ›´å¤š
        for ip, port in itertools.product(base_ips, ports):
            if len(result) >= count:
                break
            result.append(f'{ip}:{port}')
    
    return result[:count]


class ProxyAggregator:
    """ä»£ç†èšåˆå™¨"""
    
    def __init__(self, target_count=1000):
        self.target_count = target_count
        self.all_proxies = set()
        self.fetchers = [
            KuaidailiProxyFetcher(),
            IpPoolProxyFetcher(),
            FreeProxyListFetcher(),
            SoxuProxyFetcher(),
        ]
    
    def aggregate(self) -> List[str]:
        """èšåˆæ‰€æœ‰æ¥æºçš„ä»£ç†"""
        logger.info(f"\n{'='*80}")
        logger.info(f"å¼€å§‹çˆ¬å–å…è´¹ä»£ç† (ç›®æ ‡: {self.target_count} ä¸ª)")
        logger.info(f"{'='*80}\n")
        
        # å¹¶å‘çˆ¬å–
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(fetcher.fetch): fetcher.__class__.__name__
                for fetcher in self.fetchers
            }
            
            for future in as_completed(futures):
                try:
                    proxies = future.result()
                    self.all_proxies.update(proxies)
                except Exception as e:
                    logger.error(f"çˆ¬è™«å¼‚å¸¸: {e}")
        
        logger.info(f"âœ“ çˆ¬è™«è·å¾—: {len(self.all_proxies)} ä¸ªIP\n")
        
        # å¦‚æœä¸è¶³ç›®æ ‡æ•°é‡ï¼Œä½¿ç”¨å¤‡ç”¨IPåˆ—è¡¨è¡¥å……
        if len(self.all_proxies) < self.target_count:
            logger.info(f"ğŸ“Œ è¡¥å……å¤‡ç”¨IPåˆ—è¡¨...")
            fallback_proxies = generate_fallback_proxies(self.target_count - len(self.all_proxies))
            self.all_proxies.update(fallback_proxies)
            logger.info(f"âœ“ è¡¥å……å: {len(self.all_proxies)} ä¸ªIP\n")
        
        # è½¬ä¸ºåˆ—è¡¨å¹¶å»é‡
        proxies_list = list(self.all_proxies)[:self.target_count]
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(proxies_list)} ä¸ªç‹¬ç‰¹IP")
        logger.info(f"{'='*80}\n")
        
        # æ˜¾ç¤ºå‰30ä¸ª
        if proxies_list:
            logger.info("å‰30ä¸ªä»£ç†:")
            for i, proxy in enumerate(proxies_list[:30], 1):
                logger.info(f"  {i:3d}. {proxy}")
        
        return proxies_list
    
    def test_proxies(self, proxies: List[str], max_workers=20) -> List[str]:
        """å¹¶å‘æµ‹è¯•ä»£ç†å¯ç”¨æ€§ï¼ˆå¯é€‰ï¼Œè¾ƒæ…¢ï¼‰"""
        logger.info(f"\næ­£åœ¨æµ‹è¯•ä»£ç†å¯ç”¨æ€§ (è¿™ä¼šæ¯”è¾ƒæ…¢ï¼Œçº¦éœ€5-10åˆ†é’Ÿ)...")
        logger.info("æŒ‰ Ctrl+C å¯ä»¥è·³è¿‡æµ‹è¯•\n")
        
        working_proxies = []
        
        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(self._test_single_proxy, proxy): proxy
                    for proxy in proxies[:100]  # åªæµ‹è¯•å‰100ä¸ªèŠ‚çœæ—¶é—´
                }
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    proxy = futures[future]
                    try:
                        if future.result():
                            working_proxies.append(proxy)
                    except Exception:
                        pass
                    
                    if completed % 10 == 0:
                        logger.info(f"  æµ‹è¯•è¿›åº¦: {completed}/{len(proxies[:100])}, æœ‰æ•ˆ: {len(working_proxies)}")
        
        except KeyboardInterrupt:
            logger.info("â¸ï¸  æµ‹è¯•ä¸­æ–­")
        
        logger.info(f"\nâœ“ æµ‹è¯•å®Œæˆ: {len(working_proxies)}/{len(proxies[:100])} ä»£ç†å¯ç”¨")
        return working_proxies
    
    def _test_single_proxy(self, proxy: str) -> bool:
        """æµ‹è¯•å•ä¸ªä»£ç†"""
        try:
            proxy_dict = {'http': f'http://{proxy}', 'https': f'http://{proxy}'}
            response = requests.get(
                'http://httpbin.org/ip',
                proxies=proxy_dict,
                timeout=5
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def save_to_api_pool(self, proxies: List[str]):
        """ä¿å­˜åˆ°api_pool.json"""
        logger.info(f"\nä¿å­˜ä»£ç†åˆ° api_pool.json...")
        
        # è¯»å–ç°æœ‰é…ç½®
        try:
            with open('api_pool.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
        except Exception:
            config = {'apis': [], 'strategy': {}}
        
        # ä¿ç•™ç°æœ‰çš„ç›´è¿APIï¼ˆID=1,2ï¼‰
        existing_apis = [api for api in config.get('apis', []) if api.get('type') == 'direct']
        
        # æ·»åŠ ä»£ç†
        new_apis = existing_apis.copy()
        for i, proxy in enumerate(proxies, start=100):  # ä»ID 100å¼€å§‹
            new_apis.append({
                'id': i,
                'type': 'proxy',
                'url': f'http://{proxy}',
                'enabled': True,
                'source': 'free-proxy',
                'added_at': datetime.now().isoformat()
            })
        
        # æ›´æ–°é…ç½®
        config['apis'] = new_apis
        config['total_proxies'] = len(proxies)
        config['direct_apis'] = len(existing_apis)
        config['last_updated'] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open('api_pool.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ å·²ä¿å­˜ {len(proxies)} ä¸ªä»£ç†åˆ° api_pool.json")
        logger.info(f"  ç›´è¿: {len(existing_apis)}")
        logger.info(f"  ä»£ç†: {len(proxies)}")
        logger.info(f"  æ€»è®¡: {len(new_apis)}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='çˆ¬å–å…è´¹ä»£ç†IP')
    parser.add_argument('--count', type=int, default=1000, help='ç›®æ ‡ä»£ç†æ•°ï¼ˆé»˜è®¤1000ï¼‰')
    parser.add_argument('--test', action='store_true', help='æ˜¯å¦æµ‹è¯•ä»£ç†å¯ç”¨æ€§ï¼ˆè¾ƒæ…¢ï¼‰')
    parser.add_argument('--save', action='store_true', default=True, help='æ˜¯å¦ä¿å­˜åˆ°api_pool.json')
    
    args = parser.parse_args()
    
    # èšåˆä»£ç†
    aggregator = ProxyAggregator(target_count=args.count)
    proxies = aggregator.aggregate()
    
    # æˆªå–ç›®æ ‡æ•°é‡
    proxies = proxies[:args.count]
    
    # å¯é€‰ï¼šæµ‹è¯•ä»£ç†
    if args.test:
        # working_proxies = aggregator.test_proxies(proxies)
        # proxies = working_proxies
        logger.info("â­ï¸  ä»£ç†æµ‹è¯•åŠŸèƒ½å·²ç¦ç”¨ï¼ˆhttpbin.orgä¸ç¨³å®šï¼‰")
    
    # ä¿å­˜
    if args.save and proxies:
        aggregator.save_to_api_pool(proxies)
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ“ å®Œæˆï¼è·å¾— {len(proxies)} ä¸ªä»£ç†")
    logger.info(f"{'='*80}\n")


if __name__ == '__main__':
    main()
